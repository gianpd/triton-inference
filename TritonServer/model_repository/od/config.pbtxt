### ROCK NJ: OD protobuf config file


name: "od"
platform: "tensorflow_savedmodel"
default_model_filename: "model.savedmodel"
max_batch_size: 0

# input: [
#     {
#         name: "input_tensor",
#         data_type: TYPE_UINT8,
#         dims: [1,-1,-1,3]
#     }
# ]
# output: [
#     {
#         name: "detection_multiclass_scores",
#         data_type: TYPE_FP32,
#         dims: [1,100,1],
#     },
#     {
#         name: "detection_anchor_indices",
#         data_type: TYPE_FP32,
#         dims: [1,100]
#     },
#     {
#         name: "raw_detection_scores",
#         data_type: TYPE_FP32,
#         dims: [1,76725,1]
#     },
#     {
#         name: "detection_scores",
#         data_type: TYPE_FP32,
#         dims: [1,100]
#     },
#     {
#         name: "raw_detection_boxes",
#         data_type: TYPE_FP32,
#         dims: [1,76725,4]
#     },
#     {
#         name: "detection_boxes",
#         data_type: TYPE_FP32
#         dims: [1,100,4]
#     },
#     {
#         name: "num_detections",
#         data_type: TYPE_FP32,
#         dims: [1]
#     },
#     {
#         name: "detection_classes",
#         data_type: TYPE_FP32,
#         dims: [1,100]
#     }
# ]

# Switch to CPU instance since memory might not be enough for
# certain Models.

# Specify CPU instance.
#instance_group {
#  name: "od_0"
 # count: 1
  #kind: KIND_CPU
#}

# Specify GPU instance.
instance_group {
   name: "od_0"
   count: 1
   gpus: 0
   kind: KIND_GPU
 }

# Enable TensorRT acceleration running in gpu instance. It might take several
# minutes during intialization to generate tensorrt online caches.
# Limit max_workspace_size_bytes to ~256MB
optimization { execution_accelerators {
  gpu_execution_accelerator {
    name : "tensorrt"
    parameters { key: "precision_mode" value: "FP16" }
    # parameters { key: "max_workspace_size_bytes" value: "268435456"}
    # parameters { key: "minimum_segment_size" value: "10"}
}
}}

